{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eBlwjo5kTF3W"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from  torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(texts):\n",
        "  tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordPieceTrainer(\n",
        "      vocab_size=50000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<sos>\", \"<eos>\"]\n",
        "  )\n",
        "  tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "OCz4tclVVHtF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorize the data to prepare for training\n",
        "def tensorize_data(text_data, tokenizer):\n",
        "    # token index the data (i.e., numericalize)\n",
        "    numericalized_data = [\n",
        "        torch.tensor(tokenizer.encode(text).ids) for text in text_data\n",
        "    ]\n",
        "    # pad the sequences so they are all the same length (default is 0)\n",
        "    padded_data = pad_sequence(numericalized_data, batch_first=True)\n",
        "\n",
        "    # return shape (batch_size, max_len)\n",
        "    return padded_data\n"
      ],
      "metadata": {
        "id": "CjbXLhY9VjEI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text data\n",
        "texts = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I am fine, thank you!\",\n",
        "    \"Hello, how is everything?\"\n",
        "]\n",
        "\n",
        "# Train the tokenizer on the example data\n",
        "tokenizer = train_tokenizer(texts)\n",
        "\n",
        "# Convert the texts to padded tensors\n",
        "padded_tensors = tensorize_data(texts, tokenizer)\n",
        "\n",
        "# Display the resulting tensors\n",
        "print(padded_tensors)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe7mjvzjXKV6",
        "outputId": "8ca2a8ac-1fad-499e-c767-3f0341225cc5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[53,  8, 54, 67, 55,  9,  0],\n",
            "        [11, 57, 69,  8, 70, 55,  7],\n",
            "        [53,  8, 54, 60, 73,  9,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now Create the dataset domain model\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self,src_data,trg_data):\n",
        "    self.src_data = src_data\n",
        "    self.trg_data = trg_data\n",
        "  def __len__(self):\n",
        "    return len(self.src_data)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.src_data[idx],self.trg_data[idx]\n"
      ],
      "metadata": {
        "id": "RwhX_CX_ZM88"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n"
      ],
      "metadata": {
        "id": "yksdysT2ZxSF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=None):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0.0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "Y3oW_0ylby2y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        # Instantiate the linear transformation layers for Q, K, and V\n",
        "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Return both the attention output and the attention weights\n",
        "        return self.attention(x, x, x)"
      ],
      "metadata": {
        "id": "kXVxvqbsb9Vt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example input: (sequence length, batch size, model dimension)\n",
        "x = torch.rand(10, 32, 512)  # A batch of 32 sequences, each of length 10 and dimensionality 512\n",
        "\n",
        "# Initialize the multi-head self-attention layer\n",
        "attention_layer = MultiHeadSelfAttention(d_model=512, nhead=8)\n",
        "\n",
        "# Forward pass\n",
        "output, attention_weights = attention_layer(x)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # Should be (10, 32, 512)\n",
        "print(\"Attention weights shape:\", attention_weights.shape)  # Should be (32, 8, 10, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChOkm_qg7jhO",
        "outputId": "57fdba63-0626-437b-bfa6-6b0d6c99cb9d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([10, 32, 512])\n",
            "Attention weights shape: torch.Size([32, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FFN\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        # Instantiate FFN layers and dropout\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply linear transformation and ReLU non-linearity with dropout\n",
        "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "07kYwPut7kFC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Stack\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Instantiate the Multi-Head Attention and FFN layers\n",
        "        self.self_attn = MultiHeadSelfAttention(d_model, nhead)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        # Instantiate layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # transpose x to match the shape expected by the self-attention layer\n",
        "        x = x.transpose(0, 1)\n",
        "        # Apply the self-attention layer\n",
        "        attn_output, _ = self.self_attn(x)\n",
        "        # Apply dropout and layer normalization\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "        # Apply the FFN layer\n",
        "        ff_output = self.feed_forward(x)\n",
        "        # Apply dropout and layer normalization\n",
        "        x = x + self.dropout(ff_output)\n",
        "        # Transpose x back to its original shape\n",
        "        return self.norm2(x).transpose(0, 1)\n"
      ],
      "metadata": {
        "id": "TcNaTv_-83Ol"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Instantiate the Embeddings and Positional Encoding layers\n",
        "        self.embedding = Embeddings(d_model, vocab_size)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, nhead, d_ff) for _ in range(num_layers)]\n",
        "        )\n",
        "        # Define the model hyperparameters\n",
        "        self.d_model = d_model  # Embedding dimension\n",
        "        self.nhead = nhead  # Number of attention heads\n",
        "        # Define the FFN hyperparameters and Instantiate the FFN layer\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the Embeddings and Positional Encoding layers\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Mfnl1kCS9adG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Stack\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Instantiate the Multi-Head Attention and FFN layers\n",
        "        self.self_attn = MultiHeadSelfAttention(d_model, nhead)\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, nhead)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        # Instantiate layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        # Transpose x and memory to match the shape expected by the self-attention layer\n",
        "        x = x.transpose(0, 1)\n",
        "        memory = memory.transpose(0, 1)\n",
        "        # Apply the self-attention layer\n",
        "        attn_output, _ = self.self_attn(x)\n",
        "        # Apply dropout and layer normalization\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "        attn_output, _ = self.cross_attn(x, memory, memory)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm2(x)\n",
        "        # Apply the FFN layer\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        # Transpose x back to its original shape\n",
        "        return self.norm3(x).transpose(0, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "s7zHHNgo-Hkd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Instantiate the Embeddings and Positional Encoding layers\n",
        "        self.embedding = Embeddings(d_model, vocab_size)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(d_model, nhead, d_ff) for _ in range(num_layers)]\n",
        "        )\n",
        "        # Instantiate the linear transformation and softmax function\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        # Apply the Embeddings and Positional Encoding layers\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, memory)\n",
        "        # Apply the linear transformation and softmax function\n",
        "        x = self.linear(x)\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "7T5fW1DLCzNu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,d_model,nhead,d_ff,num_encoded_layers,num_decoded_layers,src_vocab_size,tgt_vocab_size,max_len):\n",
        "    super(Transformer,self).__init__()\n",
        "    self.encoder=Encoder(d_model,nhead,d_ff,num_encoded_layers,src_vocab_size,max_len)\n",
        "    self.decoder=Decoder(d_model,nhead,d_ff,num_decoded_layers,tgt_vocab_size,max_len)\n",
        "\n",
        "  def forward(self,src,tgt):\n",
        "      #Apply the Encoder and Decoder\n",
        "      memory = self.encoder(src)\n",
        "      output = self.decoder(tgt,memory)\n",
        "      return output\n"
      ],
      "metadata": {
        "id": "GLSSFw1ZC4A4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, loss_fn, optimizer, NUM_EPOCHS=10):\n",
        "    # Iterate through epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for (\n",
        "            batch\n",
        "        ) in (\n",
        "            batch_iterator\n",
        "        ):  # Assume batch_iterator yields batches of tokenized and numericalized text\n",
        "            src, tgt = batch\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            # Call the model\n",
        "            output = model(src, tgt)\n",
        "            # Compute the loss\n",
        "            loss = loss_fn(output.view(-1, TGT_VOCAB_SIZE), tgt.view(-1))\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            # Update total loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print the loss every epoch\n",
        "        print(f\"Epoch {epoch}, Loss {total_loss / len(batch_iterator)}\")"
      ],
      "metadata": {
        "id": "6F0M6chkFli0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, src_text, src_tokenizer, tgt_tokenizer, max_target_length=50):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and numericalize the source text\n",
        "    src_tokens = src_tokenizer.encode(src_text).ids\n",
        "    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Define the SOS and EOS token indices for the target vocabulary\n",
        "    tgt_sos_idx = tgt_tokenizer.token_to_id(\"<sos>\")\n",
        "    tgt_eos_idx = tgt_tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "    # Initialize the target tensor with the SOS token index\n",
        "    tgt_tensor = torch.LongTensor([tgt_sos_idx]).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Loop until the maximum target length is reached or the EOS token is generated\n",
        "    for i in range(max_target_length):\n",
        "        # Call the model to generate the output\n",
        "        with torch.no_grad():  # Disable gradient calculation to save memory during inference\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "\n",
        "        # Retrieve the predicted token\n",
        "        predicted_token_idx = output.argmax(dim=2)[0, -1].item()\n",
        "        # Check if the predicted token is the EOS token\n",
        "        if predicted_token_idx == tgt_eos_idx:\n",
        "            break\n",
        "        # Concatenate the predicted token to the target tensor\n",
        "        tgt_tensor = torch.cat(\n",
        "            (tgt_tensor, torch.LongTensor([[predicted_token_idx]])), dim=1\n",
        "        )\n",
        "\n",
        "    # Convert the target tensor to a list of token indices, decode to tokens, and join to form the translated text\n",
        "    translated_token_ids = tgt_tensor[0, 1:].tolist()  # Exclude the SOS token\n",
        "    translated_text = tgt_tokenizer.decode(\n",
        "        translated_token_ids\n",
        "    )  # Convert token ids to text\n",
        "\n",
        "    return translated_text"
      ],
      "metadata": {
        "id": "JpuO2RigFu97"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    from dataclasses import dataclass\n",
        "\n",
        "    # Instructions:\n",
        "    # Run the script with the following command: python original_transformer.py\n",
        "    # Ensure to have the data.csv file in the same directory as this script\n",
        "\n",
        "    # DEFINE HYPERPARAMETERS\n",
        "    @dataclass\n",
        "    class ConfigHyperparams:\n",
        "        # Number of layers in the encoder and decoder\n",
        "        NUM_ENCODER_LAYERS = 2\n",
        "        NUM_DECODER_LAYERS = 2\n",
        "\n",
        "        # Dropout rate\n",
        "        DROPOUT_RATE = 0.1\n",
        "\n",
        "        # Model dimensionality\n",
        "        EMBEDDING_DIM = 512\n",
        "\n",
        "        # Number of attention heads\n",
        "        NHEAD = 8\n",
        "\n",
        "        # Feed-forward network hidden dimensionality\n",
        "        FFN_HID_DIM = 2048\n",
        "\n",
        "        # Batch size\n",
        "        BATCH_SIZE = 31\n",
        "\n",
        "        # Learning rate\n",
        "        LEARNING_RATE = 0.001\n",
        "\n",
        "        # maximum length of the sequence\n",
        "        MAX_LEN = 100\n",
        "\n",
        "        # Number of epochs\n",
        "        NUM_EPOCHS = 10\n",
        "\n",
        "        def set_vocab_sizes(self, src_vocab_size, tgt_vocab_size):\n",
        "            self.SRC_VOCAB_SIZE = src_vocab_size\n",
        "            self.TGT_VOCAB_SIZE = tgt_vocab_size\n",
        "\n",
        "    # Instantiate the hyperparameters\n",
        "    hp = ConfigHyperparams()\n",
        "\n",
        "    # Load demo data\n",
        "    data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "    # Arbitrarily cap at 100 characters for demonstration to avoid long training times\n",
        "    def demo_limit(vocab, limit=hp.MAX_LEN):\n",
        "        return [i[:limit] for i in vocab]\n",
        "\n",
        "    # Separate English and French lexicons\n",
        "    EN_TEXT = demo_limit(data.en.to_numpy().tolist())\n",
        "    FR_TEXT = demo_limit(data.fr.to_numpy().tolist())\n",
        "\n",
        "    # Instantiate the tokenizer\n",
        "    en_tokenizer = train_tokenizer(EN_TEXT)\n",
        "    fr_tokenizer = train_tokenizer(FR_TEXT)\n",
        "\n",
        "    # Establish the vocabulary size\n",
        "    SRC_VOCAB_SIZE = len(en_tokenizer.get_vocab())\n",
        "    TGT_VOCAB_SIZE = len(fr_tokenizer.get_vocab())\n",
        "\n",
        "    hp.set_vocab_sizes(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE)\n",
        "\n",
        "    # Numericalize and tensorize the data\n",
        "    # Source tensor with dimensions (batch_size, max_len)\n",
        "    src_tensor = tensorize_data(EN_TEXT, en_tokenizer)\n",
        "    # Target tensor with dimensions (batch_size, max_len)\n",
        "    tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)\n",
        "\n",
        "    # Instantiate the dataset\n",
        "    dataset = TextDataset(src_tensor, tgt_tensor)\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = Transformer(\n",
        "        hp.EMBEDDING_DIM,\n",
        "        hp.NHEAD,\n",
        "        hp.FFN_HID_DIM,\n",
        "        hp.NUM_ENCODER_LAYERS,\n",
        "        hp.NUM_DECODER_LAYERS,\n",
        "        hp.SRC_VOCAB_SIZE,\n",
        "        hp.TGT_VOCAB_SIZE,\n",
        "        hp.MAX_LEN,\n",
        "    )\n",
        "    # Define the loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hp.LEARNING_RATE)\n",
        "\n",
        "    # Instantiate the batch iterator, dropping the last batch to ensure all batches are the same size\n",
        "    batch_iterator = DataLoader(\n",
        "        dataset, batch_size=hp.BATCH_SIZE, shuffle=True, drop_last=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    train(model, loss_fn, optimizer, NUM_EPOCHS=hp.NUM_EPOCHS)\n",
        "\n",
        "    # Translate a sample sentence\n",
        "    src_text = \"hello, how are you?\"\n",
        "    translated_text = translate(model, src_text, en_tokenizer, fr_tokenizer)\n",
        "    print(\"Source text:\", src_text)\n",
        "    print(\"Translated text:\", translated_text)"
      ],
      "metadata": {
        "id": "-1PfwhfbF4uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tgbUnYQ8GBUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}